<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://leomaglanoc.github.io/feed/blogs.xml" rel="self" type="application/atom+xml"/><link href="https://leomaglanoc.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-01T11:29:01+00:00</updated><id>https://leomaglanoc.github.io/feed/blogs.xml</id><title type="html">blank | Blogs</title><subtitle>Leonardo Maglanoc&apos;s personal website on robotics, AI, and humanoid robots. </subtitle><entry><title type="html">AI Coding Agent Case Study</title><link href="https://leomaglanoc.github.io/blog/AI-coding-agent-case-study/" rel="alternate" type="text/html" title="AI Coding Agent Case Study"/><published>2025-12-28T00:00:00+00:00</published><updated>2025-12-28T00:00:00+00:00</updated><id>https://leomaglanoc.github.io/blog/AI-coding-agent-case-study</id><content type="html" xml:base="https://leomaglanoc.github.io/blog/AI-coding-agent-case-study/"><![CDATA[<p>I’ve been experimenting with Codex, OpenAI’s coding agent (GPT-5.2-Codex with high reasoning effort). Below are two use cases I experimented with:</p> <ul> <li>Gaming WebApps</li> <li>Mathematical proof formalizers</li> </ul> <p>I ‘vibecoded’ in less than an hour without writing a single code line and having any previous experience in both areas. I additionally used ChatGPT for brainstorming. I’m amazed by the progress and capabilities of generative, agentic AI, even though they have some limitations too.</p> <ul> <li><a href="/assets/interactive/flappy/index.html">Flappy Bird</a></li> <li><a href="/assets/interactive/race/index.html">Top Down Racing Game</a></li> <li><a href="/assets/interactive/robot-runner/index.html">Jump and Run Sidescroller</a></li> </ul> <p>everything below here is written by GenAI!</p> <h3 id="a-tiny-but-useful-discrete-time-barrier-lemma--formalized-in-lean">A tiny (but useful) discrete-time barrier lemma — formalized in Lean</h3> <p>Barrier functions show up everywhere in safe AI and robotics: you define a scalar <strong>safety measure</strong> \(B(x) \le 0\) and you want to guarantee the system <strong>stays safe over time</strong>.</p> <p>In discrete time, we can phrase this in the simplest possible way:</p> <ul> <li>Let $x_k$ be a trajectory (states at time steps $k \in \mathbb{N}$).</li> <li>Let $B : \alpha \to \mathbb{R}$ be a barrier / safety score.</li> <li>Define the scalar sequence \(b_k := B(x_k).\)</li> <li>Assume the barrier score never increases: \(b_{k+1} \le b_k \quad \forall k.\)</li> <li>And assume we start safe: \(b_0 \le 0.\)</li> </ul> <p><strong>Claim (forward invariance in discrete time):</strong> \(\forall k,\quad b_k \le 0 \quad\text{(equivalently, } \forall k,\ B(x_k)\le 0\text{).}\)</p> <hr/> <h4 id="proof-idea-one-paragraph">Proof idea (one paragraph)</h4> <p>From $b_{k+1} \le b_k$, the sequence is <strong>non-increasing</strong>. By repeatedly chaining inequalities, \(b_k \le b_{k-1} \le \cdots \le b_0.\) So $b_k \le b_0$ for all $k$. Combining with $b_0 \le 0$ gives \(b_k \le 0 \quad \forall k.\) That’s it: <strong>non-increasing barrier value + safe start ⇒ always safe</strong>.</p> <hr/> <h4 id="lean-formalization">Lean formalization</h4> <p>Below is a Lean 4 proof (mathlib) that compiles and checks the argument mechanically.</p> <div class="language-lean highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="n">Mathlib</span><span class="o">.</span><span class="n">Data</span><span class="o">.</span><span class="n">Real</span><span class="o">.</span><span class="n">Basic</span>

<span class="k">theorem</span> <span class="n">barrier_discrete_scalar</span>
  (<span class="n">b</span> : <span class="o">ℕ</span> <span class="o">→</span> <span class="err">ℝ</span>)
  (<span class="n">h_step</span> : <span class="o">∀</span> <span class="n">k</span> : <span class="o">ℕ</span>,
    <span class="n">b</span> (<span class="n">k</span> <span class="o">+</span> <span class="mi">1</span>) <span class="o">≤</span> <span class="n">b</span> <span class="n">k</span>)
  (<span class="n">h0</span> : <span class="n">b</span> <span class="mi">0</span> <span class="o">≤</span> <span class="mi">0</span>) :
  <span class="o">∀</span> <span class="n">k</span> : <span class="o">ℕ</span>, <span class="n">b</span> <span class="n">k</span> <span class="o">≤</span> <span class="mi">0</span> := <span class="k">by</span>
  <span class="n">intro</span> <span class="n">k</span>

  <span class="k">have</span> <span class="n">hk0</span> : <span class="n">b</span> <span class="n">k</span> <span class="o">≤</span> <span class="n">b</span> <span class="mi">0</span> := <span class="k">by</span>
    <span class="n">induction</span> <span class="n">k</span> <span class="k">with</span>
    <span class="o">|</span> <span class="n">zero</span> <span class="o">=&gt;</span>
        <span class="n">exact</span> <span class="n">le_rfl</span>
    <span class="o">|</span> <span class="n">succ</span> <span class="n">k</span> <span class="n">ih</span> <span class="o">=&gt;</span>
        <span class="n">exact</span> <span class="n">le_trans</span> (<span class="n">h_step</span> <span class="n">k</span>) <span class="n">ih</span>

  <span class="n">exact</span> <span class="n">le_trans</span> <span class="n">hk0</span> <span class="n">h0</span>

<span class="k">theorem</span> <span class="n">barrier_discrete</span>
  (<span class="n">B</span> : α <span class="o">→</span> <span class="err">ℝ</span>)
  (<span class="n">x</span> : <span class="o">ℕ</span> <span class="o">→</span> α)
  (<span class="n">h_step</span> : <span class="o">∀</span> <span class="n">k</span> : <span class="o">ℕ</span>,
    <span class="n">B</span> (<span class="n">x</span> (<span class="n">k</span> <span class="o">+</span> <span class="mi">1</span>)) <span class="o">≤</span> <span class="n">B</span> (<span class="n">x</span> <span class="n">k</span>))
  (<span class="n">h0</span> : <span class="n">B</span> (<span class="n">x</span> <span class="mi">0</span>) <span class="o">≤</span> <span class="mi">0</span>) :
  <span class="o">∀</span> <span class="n">k</span> : <span class="o">ℕ</span>, <span class="n">B</span> (<span class="n">x</span> <span class="n">k</span>) <span class="o">≤</span> <span class="mi">0</span> := <span class="k">by</span>
  <span class="n">intro</span> <span class="n">k</span>

  <span class="n">simpa</span> <span class="k">using</span>
    (<span class="n">barrier_discrete_scalar</span>
      (<span class="n">b</span> := <span class="k">fun</span> <span class="n">k</span> <span class="o">=&gt;</span> <span class="n">B</span> (<span class="n">x</span> <span class="n">k</span>))
      <span class="n">h_step</span>
      <span class="n">h0</span>
      <span class="n">k</span>)
</code></pre></div></div>]]></content><author><name></name></author><category term="blogpost"/><category term="game"/><category term="canvas"/><category term="javascript"/><summary type="html"><![CDATA[I’ve been experimenting with Codex, OpenAI’s coding agent (GPT-5.2-Codex with high reasoning effort). Below are two use cases I experimented with:]]></summary></entry><entry><title type="html">My Mission</title><link href="https://leomaglanoc.github.io/blog/research-goals/" rel="alternate" type="text/html" title="My Mission"/><published>2025-12-17T00:00:00+00:00</published><updated>2025-12-17T00:00:00+00:00</updated><id>https://leomaglanoc.github.io/blog/research-goals</id><content type="html" xml:base="https://leomaglanoc.github.io/blog/research-goals/"><![CDATA[<p>My mission is to enable safe and reliable humanoid autonomy in human environments for the benefit of humanity, and to leave the world better than I found it.</p> <h2 id="near-term-research">Near-Term Research</h2> <p>Recent progress with foundation models suggests that we may be close to achieving general humanoid autonomy. However, current real-world deployment will rely on human supervision, similar to safety teleoperation in autonomous driving. Despite massive long-term investments (Google has invested ~$30 billion in Waymo over the last 15 years), fully unsupervised autonomy has not been achieved.</p> <p><a href="https://www.youtube.com/watch?v=aR20FWCCjAs">As Ilya Sutskever remarked</a>, we’re shifting from the “age of scaling” back into an “age of research”. Simply investing more compute or capital is not enough. Instead, we need research on fundamental algorithmic breakthroughs.</p> <p>To achieve my goals, I’m tackling the following subproblems:</p> <ul> <li>Safety and reliability guarantees with safe control methods</li> <li>Unify manipulation and locomotion in one safe loco-manipulation framework</li> <li>Semantic reasoning with foundation models</li> <li>Testing safe humanoids in real-world industrial and domestic use cases.</li> </ul> <p>In parallel, I’m interested in deeper theoretical questions that may inform long-term humanoid intelligence:</p> <ul> <li>Algorithmic foundations of AI agents with neuroscience insights, e.g., continual learning with scalable brain-inspired architectures</li> <li>Mathematical theory of AI</li> </ul> <p>Overall, my work spans the full technology pipeline of intelligent humanoid systems:</p> <div class="text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/pipeline-480.webp 480w,/assets/img/pipeline-800.webp 800w,/assets/img/pipeline-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/pipeline.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 900px; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <h2 id="long-term-vision">Long-Term Vision</h2> <p>Robotics &amp; AI can be both a force for <a href="https://www.darioamodei.com/essay/machines-of-loving-grace">good</a> and for <a href="https://www.darioamodei.com/essay/the-adolescence-of-technology">harm</a>. Already today, LLMs lower barriers to building <a href="https://red.anthropic.com/2025/biorisk/">dangerous systems</a> and agentic AI can automate access to harmful resources. As AI capabilities continue to improve, these risks will increasingly extend into the physical world.</p> <p>While there are growing efforts in AI safety, physical AI safety, e.g., for autonomous robots operating in human environments, remains underdeveloped.</p> <p>My long-term vision is to found a research-first robotics startup in Europe dedicated to establishing certifiable safety standards for physical AI, spanning humanoid robots, drones, and more. This will require collaboration between academia, industry, and regulators to set benchmarks for safe embodied AI.</p>]]></content><author><name></name></author><category term="blogpost"/><category term="research"/><summary type="html"><![CDATA[My mission is to enable safe and reliable humanoid autonomy in human environments for the benefit of humanity, and to leave the world better than I found it.]]></summary></entry></feed>