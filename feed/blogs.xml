<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://leomaglanoc.github.io/feed/blogs.xml" rel="self" type="application/atom+xml"/><link href="https://leomaglanoc.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-22T09:39:43+00:00</updated><id>https://leomaglanoc.github.io/feed/blogs.xml</id><title type="html">blank | Blogs</title><subtitle>Leonardo Maglanoc&apos;s personal website on robotics, AI, and humanoid robots. </subtitle><entry><title type="html">AI Coding Agent Case Study</title><link href="https://leomaglanoc.github.io/blog/AI-coding-agent-case-study/" rel="alternate" type="text/html" title="AI Coding Agent Case Study"/><published>2025-12-28T00:00:00+00:00</published><updated>2025-12-28T00:00:00+00:00</updated><id>https://leomaglanoc.github.io/blog/AI-coding-agent-case-study</id><content type="html" xml:base="https://leomaglanoc.github.io/blog/AI-coding-agent-case-study/"><![CDATA[<p>I’ve been experimenting with Codex, OpenAI’s coding agent (GPT-5.2-Codex with high reasoning effort). Below are two use cases I experimented with:</p> <ul> <li>Gaming WebApps</li> <li>Mathematical proof formalizers</li> </ul> <p>I ‘vibecoded’ in less than an hour without writing a single code line and having any previous experience in both areas. I additionally used ChatGPT for brainstorming. I’m amazed by the progress and capabilities of generative, agentic AI, even though they have some limitations too.</p> <ul> <li><a href="/assets/interactive/flappy/index.html">Flappy Bird</a></li> <li><a href="/assets/interactive/race/index.html">Top Down Racing Game</a></li> <li><a href="/assets/interactive/robot-runner/index.html">Jump and Run Sidescroller</a></li> </ul> <p>everything below here is written by GenAI!</p> <h3 id="a-tiny-but-useful-discrete-time-barrier-lemma--formalized-in-lean">A tiny (but useful) discrete-time barrier lemma — formalized in Lean</h3> <p>Barrier functions show up everywhere in safe AI and robotics: you define a scalar <strong>safety measure</strong> \(B(x) \le 0\) and you want to guarantee the system <strong>stays safe over time</strong>.</p> <p>In discrete time, we can phrase this in the simplest possible way:</p> <ul> <li>Let $x_k$ be a trajectory (states at time steps $k \in \mathbb{N}$).</li> <li>Let $B : \alpha \to \mathbb{R}$ be a barrier / safety score.</li> <li>Define the scalar sequence \(b_k := B(x_k).\)</li> <li>Assume the barrier score never increases: \(b_{k+1} \le b_k \quad \forall k.\)</li> <li>And assume we start safe: \(b_0 \le 0.\)</li> </ul> <p><strong>Claim (forward invariance in discrete time):</strong> \(\forall k,\quad b_k \le 0 \quad\text{(equivalently, } \forall k,\ B(x_k)\le 0\text{).}\)</p> <hr/> <h4 id="proof-idea-one-paragraph">Proof idea (one paragraph)</h4> <p>From $b_{k+1} \le b_k$, the sequence is <strong>non-increasing</strong>. By repeatedly chaining inequalities, \(b_k \le b_{k-1} \le \cdots \le b_0.\) So $b_k \le b_0$ for all $k$. Combining with $b_0 \le 0$ gives \(b_k \le 0 \quad \forall k.\) That’s it: <strong>non-increasing barrier value + safe start ⇒ always safe</strong>.</p> <hr/> <h4 id="lean-formalization">Lean formalization</h4> <p>Below is a Lean 4 proof (mathlib) that compiles and checks the argument mechanically.</p> <div class="language-lean highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="n">Mathlib</span><span class="o">.</span><span class="n">Data</span><span class="o">.</span><span class="n">Real</span><span class="o">.</span><span class="n">Basic</span>

<span class="k">theorem</span> <span class="n">barrier_discrete_scalar</span>
  (<span class="n">b</span> : <span class="o">ℕ</span> <span class="o">→</span> <span class="err">ℝ</span>)
  (<span class="n">h_step</span> : <span class="o">∀</span> <span class="n">k</span> : <span class="o">ℕ</span>,
    <span class="n">b</span> (<span class="n">k</span> <span class="o">+</span> <span class="mi">1</span>) <span class="o">≤</span> <span class="n">b</span> <span class="n">k</span>)
  (<span class="n">h0</span> : <span class="n">b</span> <span class="mi">0</span> <span class="o">≤</span> <span class="mi">0</span>) :
  <span class="o">∀</span> <span class="n">k</span> : <span class="o">ℕ</span>, <span class="n">b</span> <span class="n">k</span> <span class="o">≤</span> <span class="mi">0</span> := <span class="k">by</span>
  <span class="n">intro</span> <span class="n">k</span>

  <span class="k">have</span> <span class="n">hk0</span> : <span class="n">b</span> <span class="n">k</span> <span class="o">≤</span> <span class="n">b</span> <span class="mi">0</span> := <span class="k">by</span>
    <span class="n">induction</span> <span class="n">k</span> <span class="k">with</span>
    <span class="o">|</span> <span class="n">zero</span> <span class="o">=&gt;</span>
        <span class="n">exact</span> <span class="n">le_rfl</span>
    <span class="o">|</span> <span class="n">succ</span> <span class="n">k</span> <span class="n">ih</span> <span class="o">=&gt;</span>
        <span class="n">exact</span> <span class="n">le_trans</span> (<span class="n">h_step</span> <span class="n">k</span>) <span class="n">ih</span>

  <span class="n">exact</span> <span class="n">le_trans</span> <span class="n">hk0</span> <span class="n">h0</span>

<span class="k">theorem</span> <span class="n">barrier_discrete</span>
  (<span class="n">B</span> : α <span class="o">→</span> <span class="err">ℝ</span>)
  (<span class="n">x</span> : <span class="o">ℕ</span> <span class="o">→</span> α)
  (<span class="n">h_step</span> : <span class="o">∀</span> <span class="n">k</span> : <span class="o">ℕ</span>,
    <span class="n">B</span> (<span class="n">x</span> (<span class="n">k</span> <span class="o">+</span> <span class="mi">1</span>)) <span class="o">≤</span> <span class="n">B</span> (<span class="n">x</span> <span class="n">k</span>))
  (<span class="n">h0</span> : <span class="n">B</span> (<span class="n">x</span> <span class="mi">0</span>) <span class="o">≤</span> <span class="mi">0</span>) :
  <span class="o">∀</span> <span class="n">k</span> : <span class="o">ℕ</span>, <span class="n">B</span> (<span class="n">x</span> <span class="n">k</span>) <span class="o">≤</span> <span class="mi">0</span> := <span class="k">by</span>
  <span class="n">intro</span> <span class="n">k</span>

  <span class="n">simpa</span> <span class="k">using</span>
    (<span class="n">barrier_discrete_scalar</span>
      (<span class="n">b</span> := <span class="k">fun</span> <span class="n">k</span> <span class="o">=&gt;</span> <span class="n">B</span> (<span class="n">x</span> <span class="n">k</span>))
      <span class="n">h_step</span>
      <span class="n">h0</span>
      <span class="n">k</span>)
</code></pre></div></div>]]></content><author><name></name></author><category term="blogpost"/><category term="game"/><category term="canvas"/><category term="javascript"/><summary type="html"><![CDATA[I’ve been experimenting with Codex, OpenAI’s coding agent (GPT-5.2-Codex with high reasoning effort). Below are two use cases I experimented with:]]></summary></entry><entry><title type="html">My Mission</title><link href="https://leomaglanoc.github.io/mission" rel="alternate" type="text/html" title="My Mission"/><published>2025-12-17T00:00:00+00:00</published><updated>2025-12-17T00:00:00+00:00</updated><id>https://leomaglanoc.github.io/research-goals</id><content type="html" xml:base="https://leomaglanoc.github.io/mission"><![CDATA[<p>I work on making humanoid autonomy deployable through formal safety guarantees — from semantic intent to torque-level execution.</p> <p>Humanoid robots are becoming capable, but not yet safe enough to deploy at scale in human environments.</p> <p>My incoming PhD builds the missing layer: a policy-agnostic runtime safety shield that enforces provable guarantees from intent to torque.</p> <p>Long-term, this work aims to establish certifiable safety infrastructure for physical AI in Europe and translate formal guarantees into deployable systems.</p> <h2 id="near-term-research">Near-Term Research</h2> <p>Recent progress in foundation models suggests that general humanoid autonomy may be within reach. Yet real-world deployment still relies on human supervision, similar to safety teleoperation in autonomous driving.</p> <p>As OpenAI co-founder and former Chief Scientist Ilya Sutskever <a href="https://www.youtube.com/watch?v=aR20FWCCjAs">remarked</a>, we are shifting from the “age of scaling” back into an “age of research”. Scaling improves capability; it does not provide guarantees. What is required are algorithmic advances that introduce structure and formal guarantees.</p> <p>A central challenge is ensuring that learning-based control systems (VLA-based or RL-based) operate within certifiable safety envelopes. My work on our safety shield for human-robot interaction (<a href="https://arxiv.org/abs/2412.10180">IEEE T-RO</a>, <a href="https://youtu.be/IUAeZGau28E?si=0zWlQfw3i6It8nDD">demo</a>) demonstrates how runtime verification and failsafe planning can constrain robot behavior in human environments while preserving performance.</p> <p>The next step is extending this framework to full humanoid systems. This requires:</p> <ul> <li>Unifying manipulation and locomotion within a single safe loco-manipulation framework</li> <li>Integrating semantic safety reasoning with foundation models</li> <li>Validating safe humanoid systems in real-world industrial and domestic environments</li> </ul> <p>The objective is to make humanoid autonomy deployable at scale through formal safety guarantees.</p> <h2 id="long-term-vision">Long-Term Vision</h2> <p>Already today, LLMs lower barriers to building <a href="https://red.anthropic.com/2025/biorisk/">dangerous systems</a> and agentic AI can automate access to harmful resources. As AI capabilities continue to improve, safety risks are increasingly extending from digital systems into the physical world.</p> <p>While substantial effort has been devoted to digital AI safety, physical AI safety (e.g., for autonomous robots operating in human environments) remains comparatively underdeveloped.</p> <p>My long-term vision is to establish certifiable safety standards for physical AI in Europe and translate them into deployable infrastructure for real-world systems (e.g., humanoid robots, autonomous drones, and human-in-the-loop embodied systems such as brain-computer interfaces). For agentic systems, physical safety cannot be separated from semantic safety: robots must not only act within physical constraints, but also <a href="https://www.anthropic.com/news/claude-new-constitution">align their behavior with human-defined norms and values</a>.</p> <p>Achieving this requires tight collaboration among academia, industry, and regulators to ensure that embodied AI systems are not only capable, but certifiably safe by design.</p>]]></content><author><name></name></author><category term="blogpost"/><category term="research"/><summary type="html"><![CDATA[I work on making humanoid autonomy deployable through formal safety guarantees — from semantic intent to torque-level execution.]]></summary></entry></feed>